{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Load data from CRCNS:hc-3 into pandas DataFrame\n",
    "\n",
    "_Etienne Ackermann, 07/02/2015_\n",
    "\n",
    "This series of notebooks will go through the steps of analyzing some data from the CRCNS hc-3 data set, as well as generating some of the figures to be used on my SfN poster.\n",
    "\n",
    "The data can be downloaded from the [CRCNS](http://crcns.org/data-sets/hc/hc-3) (Collaborative Research in Computational Neuroscience) website, and the ```hc-3``` data set in particular.\n",
    "\n",
    "Here I will use a modified version of the Python module YAHMM (yet another hidden Markov model) by Jacob Schreiber.\n",
    "\n",
    "The (rough) workflow of my YAHMM experiment is as follows:\n",
    "1. load and manipule data from the hc-3 data set (from the crcns.org website) into a pandas DataFrame containing spike counts\n",
    " 1. extract single unit spike trains from data and store in a list of spike trains\n",
    " 1. bin and count the spikes and save the results in a DataFrame: `df_spk_counts`\n",
    "1. separate the observations (`df_spk_counts`) into training and a validation sets\n",
    " 1. extract sequences of random lengths (bounded by min and max lengths) and summarize sequence info\n",
    " 1. use `df_spk_counts` and sequence summary info to build sequences of observations suitable for YAHMM; store these in `training_sequences` and `validation_sequences`, respectively\n",
    "1. define and bake various hidden Markov models\n",
    "1. use training sequences to learn the parameters of the various models defined previously\n",
    "1. ...\n",
    "\n",
    "### Summary\n",
    "From the data set description page:\n",
    ">The data set contains recordings made from multiple hippocampal areas in Long-Evans rats, including: Cornu Ammonis: CA1 and CA3; dentate gyrus (DG), and entorhinal cortex: EC2, EC3, EC4, EC5.  The data was obtained during 442 recording sessions and includes responses of 7,737 neurons in eleven animals while they performed one of fourteen behaviors.  Total time for all experiments is 204.5 hours.   The raw (broadband) data was recorded at 20KHz, simultaneously from 31 to 127 channels.  The raw data was processed to extract the LFPs (Local Field Potentials) and detect spikes. Included in the data set are the following items:\n",
    "1. Times and waveforms of detected potential spikes.\n",
    "1. Results of spike sorting.\n",
    "1. LFPs.\n",
    "1. For many sessions, the coordinate and direction of the rats head and video files from which the positions were extracted.\n",
    "1. Metadata tables giving properties of the neurons and characteristics of the recording sessions.\n",
    "\n",
    "### Subset of interest\n",
    "I will start by considering **gor01** and **vvp01** (animals). In particular, they have the following number of recorded sessions:\n",
    "\n",
    "|Task|gor01|vvp01|Description\n",
    "|--|--:|--:|--|\n",
    "|linear one|3|5|linear track (250 cm?)|\n",
    "|linear two|3|5|linear track (250 cm?)|\n",
    "|open||3|open field|\n",
    "|sleep||1|sleeping|\n",
    "|T maze|2|1||\n",
    "|wheel|1||operant wheel running task; see Mizuseki et al., 2009|\n",
    "\n",
    "In this way, I might be able to distinguish between different environments using my HMM implementation. There are (perhaps) easy cases, such as linear vs wheel, and perhaps less easy cases, such as linear one vs linear two. \n",
    "\n",
    "Ideally, I would like to see\n",
    "1. small intratask variability, and\n",
    "1. large intertask variability\n",
    "\n",
    "although this remains to be seen...\n",
    "\n",
    "Also note that spike sorting occurs on a *daily* basis, so that sorted units are only meaningful within a small number of recording sessions.\n",
    "\n",
    "### Data description (technical details)\n",
    "Note that both **gor01** and **vvp01** were recorded with a Neuralynx recording system at 32,552 Hz, then amplified 1000x, followed by 1–5 kHz bandpass filtering.\n",
    "\n",
    "#### Experiment\n",
    "* .dat file. Raw data or \"wideband data.\" Contains everything–LFP (Local Field Potentials) and spikes. Has up to 32 (four shank probe) or 64 (eight shank probe) short integers (2 bytes, signed) every time step. (One integer from each recording site, i.e. channel). Actual number of channels is specified in the .xml file and is not always a multiple of 8 because bad channels (due to very high impedance or broken shank) were removed from the data.\n",
    "* .xml file. Has information about recording. Also used for viewing data using neuroscope, can be loaded into Matlab using the LoadXml script and can be viewed using ndmanager (a GUI to browse the xml file).\n",
    "* .mpg file. Video recording of animal position during the experiment. The video shows LEDs attached to the animal. This is used to generate the .whl file, which contains the coordinates of the position of the LEDs. \n",
    "* .led file. Contains recordings (at 32,552 Hz sampling rate) of the synchronization pulse that drives a stationary flashing LED visible in the .mpg file. Format is one short integer (2 bytes, signed) per sample.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting [useful] data from hc-3\n",
    "\n",
    "I will assume that the data from hc-3 have already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function saveFigure(filename) loaded\n",
      "\n",
      "Tip: to save a figure, call saveFigure(\"path/figure.pdf\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "#import yahmm as ym\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "from efunctions import * # load my helper functions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract spike trains for linear one and linear two, day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinOne': ['/home/etienne/Dropbox/neoReader/Data/gor01-6-12/2006-6-12_15-55-31_lin1/2006-6-12_15-55-31'],\n",
       " 'LinTwo': ['/home/etienne/Dropbox/neoReader/Data/gor01-6-12/2006-6-12_16-53-46_lin2/2006-6-12_16-53-46'],\n",
       " 'animal': 'gor01',\n",
       " 'datadir': '/home/etienne/Dropbox/neoReader/Data',\n",
       " 'sessions': ['gor01-6-12', 'gor01-6-13', 'gor01-6-7']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def get_sessions_for_animal(a_dir, animal):\n",
    "    return [name for name in os.listdir(a_dir) if (os.path.isdir(os.path.join(a_dir, name))) & (name[0:len(animal)]==animal)]\n",
    "\n",
    "def get_trials_for_session(SessionInfo, session=0):\n",
    "    mypath = SessionInfo['datadir'] + '/' + SessionInfo['sessions'][session]\n",
    "    SessionInfo['LinOne'] = [mypath + '/' + name + '/' + name[:-5] for name in os.listdir(mypath) if (os.path.isdir(os.path.join(mypath, name))) & (name[-4:]=='lin1')]\n",
    "    SessionInfo['LinTwo'] = [mypath + '/' + name + '/' + name[:-5] for name in os.listdir(mypath) if (os.path.isdir(os.path.join(mypath, name))) & (name[-4:]=='lin2')]\n",
    "\n",
    "\n",
    "SessionInfo=dict()\n",
    "SessionInfo['datadir'] = '/home/etienne/Dropbox/neoReader/Data'\n",
    "#SessionInfo['datadir'] = 'C:/Etienne/Dropbox/neoReader/Data'\n",
    "SessionInfo['animal'] = 'gor01'\n",
    "SessionInfo['sessions'] = get_sessions_for_animal(SessionInfo['datadir'],SessionInfo['animal'])\n",
    "get_trials_for_session(SessionInfo, session=0)\n",
    "\n",
    "SessionInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spikes recorded on each electrode is as follows:\n",
      "For linear one:\n",
      "clu1      97182\n",
      "clu10     86211\n",
      "clu11    464786\n",
      "clu12    204332\n",
      "clu2     104050\n",
      "clu3      84827\n",
      "clu4      99562\n",
      "clu5      32485\n",
      "clu6      49000\n",
      "clu7     101825\n",
      "clu8      97978\n",
      "clu9     125859\n",
      "dtype: int64\n",
      "For linear two:\n",
      "clu1      60837\n",
      "clu10     61657\n",
      "clu11    363033\n",
      "clu12    178708\n",
      "clu2      67546\n",
      "clu3      46988\n",
      "clu4      75359\n",
      "clu5      41363\n",
      "clu6      35129\n",
      "clu7      71768\n",
      "clu8      66574\n",
      "clu9     121952\n",
      "dtype: int64\n",
      "The total number of spikes detected (lin1, lin2):1548097, 1190914\n"
     ]
    }
   ],
   "source": [
    "# specify session parameters\n",
    "\n",
    "num_electrodes = 12\n",
    "\n",
    "for ele in np.arange(num_electrodes):\n",
    "    dt1a = pd.read_table( SessionInfo['LinOne'][0] + '.clu.' + str(ele + 1), skiprows=1, names='u' )\n",
    "    dt1b = pd.read_table( SessionInfo['LinOne'][0] + '.res.' + str(ele + 1), header=None, names='t' )\n",
    "    dt2a = pd.read_table( SessionInfo['LinTwo'][0] + '.clu.' + str(ele + 1), skiprows=1, names='u' )\n",
    "    dt2b = pd.read_table( SessionInfo['LinTwo'][0] + '.res.' + str(ele + 1), header=None, names='t' )\n",
    "    ls1a = list(dt1a['u'])\n",
    "    ls1b = list(dt1b['t'])\n",
    "    ls2a = list(dt2a['u'])\n",
    "    ls2b = list(dt2b['t'])\n",
    "    d1 = {'clu' + str( ele + 1 ): Series(ls1a, index=ls1b)}\n",
    "    d2 = {'clu' + str( ele + 1 ): Series(ls2a, index=ls2b)}\n",
    "    if ele == 0:\n",
    "        df1 = DataFrame(d1)\n",
    "        df2 = DataFrame(d2)\n",
    "    else:\n",
    "        df1 = df1.append(DataFrame(d1))\n",
    "        df2 = df2.append(DataFrame(d2))\n",
    "\n",
    "print( 'The number of spikes recorded on each electrode is as follows:' )\n",
    "print( 'For linear one:' )\n",
    "print( df1.count() )\n",
    "print( 'For linear two:' )\n",
    "print( df2.count() )\n",
    "print( 'The total number of spikes detected (lin1, lin2):' + str(sum(df1.count())) + ', ' + str(sum(df2.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we now need to transform the data so that we can view the spikes by sorted units, and not on which electrodes they were recorded.\n",
    "\n",
    "First, we know that units 0 and 1 should be omitted from any analyses, since these correspond to mechanical noise, and small, unsortable spikes, respectively.\n",
    "\n",
    "So let's remove all rows of our DataFrames with spikes corresponding only to clusters 0 or 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of spikes recorded on each electrode (after removing spikes corresponding to clusters 0 and 1) is as follows:\n",
      "For linear one:\n",
      "clu1         0\n",
      "clu10    29884\n",
      "clu11        0\n",
      "clu12    25596\n",
      "clu2     25508\n",
      "clu3         4\n",
      "clu4        82\n",
      "clu5         0\n",
      "clu6         0\n",
      "clu7     51037\n",
      "clu8     40370\n",
      "clu9        46\n",
      "dtype: int64\n",
      "For linear two:\n",
      "clu1         0\n",
      "clu10    20920\n",
      "clu11        0\n",
      "clu12    17930\n",
      "clu2     19262\n",
      "clu3         0\n",
      "clu4        32\n",
      "clu5         0\n",
      "clu6         0\n",
      "clu7     34926\n",
      "clu8     30873\n",
      "clu9       416\n",
      "dtype: int64\n",
      "The total number of spikes detected (lin1, lin2):172527, 124359\n"
     ]
    }
   ],
   "source": [
    "cidx = 0\n",
    "idx1 = list(df1[df1==cidx].dropna(how='all').index)\n",
    "idx2 = list(df2[df2==cidx].dropna(how='all').index)\n",
    "df1 = df1.drop(idx1)\n",
    "df2 = df2.drop(idx2)\n",
    "\n",
    "cidx = 1\n",
    "idx1 = list(df1[df1==cidx].dropna(how='all').index)\n",
    "idx2 = list(df2[df2==cidx].dropna(how='all').index)\n",
    "df1 = df1.drop(idx1)\n",
    "df2 = df2.drop(idx2)\n",
    "\n",
    "print( 'The number of spikes recorded on each electrode (after removing spikes corresponding to clusters 0 and 1) is as follows:' )\n",
    "print( 'For linear one:' )\n",
    "print( df1.count() )\n",
    "print( 'For linear two:' )\n",
    "print( df2.count() )\n",
    "print( 'The total number of spikes detected (lin1, lin2):' + str(sum(df1.count())) + ', ' + str(sum(df2.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clean-up operation has significantly reduced the size of our DataFrame, so extracting spike trains should now be a little bit faster, or at least require less memory.\n",
    "\n",
    "Note that this clean-up is not perfect. In particular, the count operation counts only numbers per column, and ignores the NaNs, UNLESS the entire column is filled with NaNs, in which case, it counts the number of NaNs. This is the case for df2.clu5, for example, where we have 5583 NaNs after clean-up, and no actual useable spikes.\n",
    "\n",
    "We can now request spike trains very similar to our clean-up operation. That is, we find all the rows in which any column has a value for the unit number that we want, and we note the timestamps. This is our spike train for that unit, although the timestamps have not been sorted yet.\n",
    "\n",
    "Let's do this now. But first, as an example, consider the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove clu5, or any other electrode without ANY spikes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_spk_cols = [ x|y for (x,y) in zip((df1.count()==0).tolist(), (df2.count()==0).tolist())]\n",
    "zspi = [i for i, elem in enumerate(zero_spk_cols) if elem]\n",
    "df1.drop(df1.columns[zspi], axis=1, inplace=True) # WARNING! df1 and df2 must have the same column order!\n",
    "df2.drop(df2.columns[zspi], axis=1, inplace=True) # WARNING! df1 and df2 must have the same column order!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clu10</th>\n",
       "      <th>clu12</th>\n",
       "      <th>clu2</th>\n",
       "      <th>clu4</th>\n",
       "      <th>clu7</th>\n",
       "      <th>clu8</th>\n",
       "      <th>clu9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clu10  clu12  clu2  clu4  clu7  clu8  clu9\n",
       "278    NaN    NaN   NaN   NaN   NaN    17   NaN\n",
       "283    NaN     10   NaN   NaN   NaN   NaN   NaN\n",
       "301    NaN    NaN   NaN   NaN     2   NaN   NaN\n",
       "537    NaN      3   NaN   NaN   NaN   NaN   NaN\n",
       "575    NaN    NaN   NaN   NaN     2   NaN   NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have called `sort_index()` (which does not replace the order in `df1` by the way), from where we can see that the first three spikes have been attributed to unit 4, and the next two spikes to unit 14.\n",
    "\n",
    "Also, since we would like to ultimately sort the spike trains by timestamp for all of the units, we may as well sort the entire DataFrame once, and then extract the (already-sorted) spike trains from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.sort_index()\n",
    "df2 = df2.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW!!! Now I rename all the units sequentially, starting on clu1 with unit 2 renamed as 1, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_units(dfextern):\n",
    "    df = dfextern.copy()\n",
    "    prevmax = 0\n",
    "    num_shanks = len(df.columns)\n",
    "    for ss in np.arange(0,num_shanks):\n",
    "        if df[df.columns[ss]].min() == 2:\n",
    "            df.loc[:,df.columns[ss]][df.loc[:,df.columns[ss]].notnull()] = df.loc[:,df.columns[ss]][df.loc[:,df.columns[ss]].notnull()] + prevmax - 1\n",
    "        prevmax = df.loc[:,df.columns[ss]].max()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1rn = rename_units(df1)\n",
    "df2rn = rename_units(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clu10     7\n",
       "clu12    16\n",
       "clu2     21\n",
       "clu4     23\n",
       "clu7     26\n",
       "clu8     45\n",
       "clu9     51\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1rn.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we can request the spike trains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_units1 = int(max( df1rn.max() ))\n",
    "num_units2 = int(max( df2rn.max() ))\n",
    "\n",
    "spk_counts1 = {}\n",
    "spk_counts2 = {}\n",
    "st_list1 = []\n",
    "st_list2 = []\n",
    "for uu in np.arange(1,num_units1+1):\n",
    "    st1 = list((((df1rn[df1rn==uu])).dropna(how='all')).index)\n",
    "    spk_counts1['u' + str(uu)] = len(st1)\n",
    "    st2 = list((((df2rn[df2rn==uu])).dropna(how='all')).index)\n",
    "    spk_counts2['u' + str(uu)] = len(st2)\n",
    "    st_list1.append(st1)\n",
    "    st_list2.append(st2)\n",
    "# convert list of lists to list of ndarrays:\n",
    "st_array1rn = [np.array(a) for a in st_list1]\n",
    "st_array2rn = [np.array(a) for a in st_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../Data/st_array1rn126.pickle', 'wb') as f:\n",
    "    pickle.dump(st_array1rn, f)\n",
    "with open('../../Data/st_array2rn126.pickle', 'wb') as f:\n",
    "    pickle.dump(st_array2rn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating over 4536 intervals...\n",
      "iterating over 3288 intervals...\n"
     ]
    }
   ],
   "source": [
    "def list_of_spk_time_arrays_to_spk_counts_arrays(st_array_extern, ds=0, fs=0 ):\n",
    "    \"\"\"\n",
    "    st_array: list of ndarrays containing spike times (in sample numbers!)\n",
    "    ds:       delta sample number; integer value of samples per time bin\n",
    "    fs:       sampling frequency\n",
    "    \n",
    "    argument logic: if only st_array is passed, use default ds; if ds is passed, use as is and ignore fs; if ds and fs are passed, use ds as time in seconds\n",
    "    \n",
    "    returns a (numBins x numCell) array with spike counts\n",
    "    \"\"\"\n",
    "    \n",
    "    st_array = st_array_extern\n",
    "    \n",
    "    if fs == 0:\n",
    "        if ds == 0:\n",
    "            ds = 1000 # assume default interval size\n",
    "    else: # sampling frequency was passed, so interpret ds as time-interval, and convert accordingly:\n",
    "        if ds == 0:\n",
    "            ds = 1000 # assume default interval size\n",
    "        else:\n",
    "            ds = round(ds*fs)\n",
    "            \n",
    "    # determine number of units:\n",
    "    num_units = len(st_array)\n",
    "    \n",
    "    #columns = np.arange(0,num_units)\n",
    "    #df = DataFrame(columns=columns)\n",
    "    \n",
    "    maxtime = 0\n",
    "    for uu in np.arange(num_units):\n",
    "        try:\n",
    "            maxtime = max(st_array[uu].max(), maxtime)\n",
    "        except:\n",
    "            maxtime = maxtime\n",
    "    \n",
    "    # create list of intervals:\n",
    "    intlist = np.arange(0,maxtime,ds)\n",
    "    num_bins = len(intlist)\n",
    "    \n",
    "    spks_bin = np.zeros((num_bins,num_units))\n",
    "    \n",
    "    print(\"iterating over {0} intervals...\".format(num_bins))\n",
    "    \n",
    "    for uu in np.arange(num_units):\n",
    "        # count number of spikes in an interval:\n",
    "        spks_bin[:,uu] = np.histogram(st_array[uu], bins=num_bins, density=False, range=(0,maxtime))[0]\n",
    "        #spk_count_list.append([x&y for (x,y) in zip(st_array[uu]>ii, st_array[uu] < ii+ds)].count(True))\n",
    "        #st_array[uu] = st_array[uu][st_array[uu]>ii+ds]        \n",
    "        #if df.empty:\n",
    "        #    df = DataFrame([spk_count_list], columns=columns)\n",
    "        #else:\n",
    "        #    df = df.append(DataFrame([spk_count_list], columns=columns),ignore_index=True)\n",
    "                    \n",
    "    return pd.DataFrame(spks_bin)\n",
    "\n",
    "df_spk_counts1 = list_of_spk_time_arrays_to_spk_counts_arrays(st_array1rn, ds=0.25, fs=32552)\n",
    "df_spk_counts2 = list_of_spk_time_arrays_to_spk_counts_arrays(st_array2rn, ds=0.25, fs=32552)\n",
    "#df_spk_counts2 = list_of_spk_time_arrays_to_spk_counts_df(st_array2, ds=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9  ...  41  42  43  44  45  46  47  48  \\\n",
       "0   0   0   0   4   0   0   0   0   1   0 ...  10   0   0   0   0   0   0   0   \n",
       "1   0   0   0   4   0   0   0   0   0   0 ...   7   0   0   4   0   0   0   0   \n",
       "2   0   0   0   9   0   0   0   0   2   0 ...   9   0   0   0   0   0   0   0   \n",
       "3   0   0   0   6   0   0   0   0   1   0 ...   9   0   0   1   0   0   0   0   \n",
       "4   0   0   0   5   0   0   0   0   2   0 ...   8   0   0   0   0   0   0   0   \n",
       "\n",
       "   49  50  \n",
       "0   0   0  \n",
       "1   0   0  \n",
       "2   0   0  \n",
       "3   0   0  \n",
       "4   0   0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spk_counts1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9  ...  41  42  43  44  45  46  47  48  \\\n",
       "0   0   0   0   1   0   0   0   0   7   0 ...   0   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "2   0   0   4   0   0   0   0   0   8   0 ...   0   0   0   0   0   0   0   0   \n",
       "3   0   0   6   5   0   0   0   0  11   0 ...   0   0   0   0   0   0   0   0   \n",
       "4   0   0   0   4   0   0   0   0   1   0 ...   0   0   0   0   0   0   0   0   \n",
       "\n",
       "   49  50  \n",
       "0   0   0  \n",
       "1   0   0  \n",
       "2   0   0  \n",
       "3   0   0  \n",
       "4   0   0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spk_counts2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After re-writing the binning function, it is now really fast to bin, so that I don't think I need to store these anymore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "# WARNING!!! Pickling does NOT preserve meta-data as per below...\n",
    "#df_spk_counts1._description = 'gor01-6-7/2006-6-7_11-26-53_lin1'\n",
    "#df_spk_counts1._timebin = '250 ms'\n",
    "#df_spk_counts2._description = 'gor01-6-7/2006-6-7_16-40-19_lin2'\n",
    "#df_spk_counts2._timebin = '250 ms'\n",
    "\n",
    "#with open('../../Data/df_spk_counts1_250ms_rn.pickle', 'wb') as f:\n",
    "#    pickle.dump(df_spk_counts1, f)\n",
    "#\n",
    "#with open('../../Data/df_spk_counts2_250ms_rn.pickle', 'wb') as f:\n",
    "#    pickle.dump(df_spk_counts2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
